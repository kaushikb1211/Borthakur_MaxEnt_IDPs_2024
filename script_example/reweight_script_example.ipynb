{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7234b7bb-5eb0-4d87-9d96-de91baa11768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from pyblock.blocking import reblock, find_optimal_block\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from numpy_indexed import group_by as group_by_\n",
    "import time\n",
    "# utility functions\n",
    "\n",
    "def group_by(keys: np.ndarray,\n",
    "             values: np.ndarray = None,\n",
    "             reduction: callable = None):\n",
    "    \n",
    "    if reduction is not None:\n",
    "        values = np.ones_like(keys) / len(keys) if values is None else values\n",
    "\n",
    "        if values.squeeze().ndim > 1:\n",
    "\n",
    "            return np.stack([i[-1] for i in group_by_(keys=keys, values=values, reduction=reduction)])\n",
    "\n",
    "        else:\n",
    "            return np.asarray(group_by_(keys=keys, values=values, reduction=reduction))[:, -1]\n",
    "\n",
    "    values = np.arange(len(keys)) if values is None else values\n",
    "\n",
    "    return group_by_(keys).split_array_as_list(values)\n",
    "\n",
    "\n",
    "def reindex_list(unsorted_list: list, indices: \"list or np.ndarray\"):\n",
    "    return list(map(unsorted_list.__getitem__, to_numpy(indices).astype(int)))\n",
    "\n",
    "\n",
    "def to_numpy(x: \"int, list or array\"):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x\n",
    "    if isinstance(x, (int, float, np.int64, np.int32, np.float32, np.float64)):\n",
    "        return np.array([x])\n",
    "    if isinstance(x, list):\n",
    "        return np.asarray(x)\n",
    "    if isinstance(x, (map, filter, tuple)):\n",
    "        return np.asarray(list(x))\n",
    "\n",
    "\n",
    "def rmse(x, y):\n",
    "    return np.sqrt(np.power(x.flatten()-y.flatten(), 2).mean())\n",
    "\n",
    "\n",
    "def block_error(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    x : (d, N) numpy array with d features and N measurments\n",
    "    \"\"\"\n",
    "    n = x.shape[-1]\n",
    "    blocks = reblock(x)\n",
    "    optimal_indices = find_optimal_block(n, blocks)\n",
    "    optimal_indices = np.asarray(find_optimal_block(n, blocks))\n",
    "    isnan = np.isnan(optimal_indices)\n",
    "    mode = Counter(optimal_indices[~isnan].astype(int)).most_common()[0][0]\n",
    "    optimal_indices[isnan] = mode\n",
    "    return np.asarray([blocks[i].std_err[j] for j, i in enumerate(optimal_indices.astype(int))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb662a41-f018-4575-8a07-f845eadb0e53",
   "metadata": {},
   "source": [
    " # Load in data and group indices by data type \n",
    " ## (in actual use case, data types would be partitioned by type, i.e. CA_shifts, RDCs, ect. in a dictionary)\n",
    "\n",
    " # Here, we're using the data for drkN - a99SBdisp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03479c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ids(ids):\n",
    "    types = np.array([\"_\".join(i.split(\"_\")[:-1]) for i in ids])\n",
    "    indices_list = group_by(types)\n",
    "    indices = reindex_list(indices_list, np.argsort(np.fromiter(map(np.mean, indices_list), int)))\n",
    "    return indices\n",
    "\n",
    "# data_type_indices = process_ids(data_ids)\n",
    "# len(data_type_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62e0db8-cac9-4e50-bad8-8e7acd008774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data array with forward model results,  dimensions : (458, 29976)\n",
      " Experimental average array, dimensions : (458,)\n"
     ]
    }
   ],
   "source": [
    "constraints = np.load('/Users/kaushikborthakur1211/Desktop/Paper_Repo/drkN/scripts/drkN_constraints.npy')\n",
    "targets = np.load('/Users/kaushikborthakur1211/Desktop/Paper_Repo/drkN/scripts/drkN_exps.npy')\n",
    "data_ids = np.load('/Users/kaushikborthakur1211/Desktop/Paper_Repo/drkN/scripts/data_ids.npy')\n",
    "sigma_md = np.load('/Users/kaushikborthakur1211/Desktop/Paper_Repo/drkN/scripts/sigma_md.npy')\n",
    "\n",
    "data_type_indices = process_ids(data_ids)\n",
    "\n",
    "print(f\"Data array with forward model results,  dimensions : {constraints.shape}\\n\", \n",
    "      f\"Experimental average array, dimensions : {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e0cca-b96c-40d6-af56-dfa8cd94b62c",
   "metadata": {},
   "source": [
    "# Clean class implementation to rewight and find optimal regularization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a54984-a6bc-4106-a99b-5ba9c92b5a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "def conditional_ray(attr):\n",
    "    \"\"\"conditional ray decorator\n",
    "    \"\"\"\n",
    "\n",
    "    def decorator(func):\n",
    "\n",
    "        def inner(*args, **kwargs):\n",
    "\n",
    "            is_ray = getattr(args[0], attr)\n",
    "\n",
    "            if is_ray:\n",
    "                return ray.remote(func)\n",
    "            else:\n",
    "                return func\n",
    "\n",
    "        return inner\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class MaxEntropyReweight():\n",
    "    def __init__(self,\n",
    "                 constraints: list,\n",
    "                 targets: list,\n",
    "                 sigma_md: list = None,\n",
    "                 sigma_reg: list = None,\n",
    "                 target_kish: float = 10):\n",
    "\n",
    "        \"\"\"\n",
    "        constraints : list of numpy arrays each with shape (N_observations, ).\n",
    "                      Each array should be paired with a target. \n",
    "                      Optimization is performed to find a set of weights (N_observations)\n",
    "                      that will result in a weighted average for each constraint that equals the corresponding target.\n",
    "\n",
    "        targets : list of targets for each constraint. \n",
    "\n",
    "        sigma_md : error of each constraint data type estimated from blocking (correlated time series data)\n",
    "\n",
    "        sigma_reg : regularization parameter for each constraint, class method optimize_sigma_reg will find these\n",
    "\n",
    "        target_kish : minimum kish required when searching for sigma_reg for each data type.\n",
    "                      Will not necessarily match the kish of the final reweighting of all constraints combined.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.constraints = np.asarray(constraints)\n",
    "        self.targets = np.asarray(targets)\n",
    "        self.lambdas0 = np.zeros(len(constraints))\n",
    "        self.n_samples = len(constraints[0])\n",
    "        self.n_constraints = len(self.lambdas0)\n",
    "\n",
    "        # regularizations\n",
    "        self.target_kish = target_kish\n",
    "\n",
    "        # result status\n",
    "        self.has_result = False\n",
    "        self.weights = None\n",
    "        self.lambdas = None\n",
    "\n",
    "        # error in comp data\n",
    "        self.sigma_md = block_error(np.asarray(constraints)) if sigma_md is None else np.copy(sigma_md)\n",
    "\n",
    "        # regularization hyperparameter (one per data type)\n",
    "        self.sigma_reg = np.zeros(self.n_constraints) if sigma_reg is None else np.copy(sigma_reg)\n",
    "\n",
    "        self.is_ray = False\n",
    "\n",
    "    def compute_weights(self, lambdas, constraints: np.ndarray = None):\n",
    "        constraints = self.constraints if constraints is None else constraints\n",
    "        logits = 1 - np.dot(constraints.T, lambdas)\n",
    "        # Normalize exponents to avoid overflow\n",
    "        weights = np.exp(logits - logits.max())\n",
    "        # return weights\n",
    "        return weights / np.sum(weights)\n",
    "\n",
    "    def compute_entropy(self, weights: np.ndarray = None, *args):\n",
    "        if weights is None:\n",
    "            assert self.weights is not None, \"Must provide weights if class attribute 'weights' is None\"\n",
    "            weights = self.weights\n",
    "        entropy = -np.sum(weights * np.log(weights + 1e-12))  # Small offset to avoid log(0)\n",
    "        return entropy\n",
    "\n",
    "    def compute_weighted_mean(self, weights: np.ndarray = None):\n",
    "        if weights is None:\n",
    "            assert self.weights is not None, \"Must provide weights if class attribute 'weights' is None\"\n",
    "            weights = self.weights\n",
    "        return self.constraints @ weights\n",
    "\n",
    "    def lagrangian(self,\n",
    "                   lambdas,\n",
    "                   constraints: np.ndarray,\n",
    "                   targets: np.ndarray,\n",
    "                   regularize: bool = False,\n",
    "                   sigma_reg: np.ndarray = None,\n",
    "                   sigma_md: np.ndarray = None):\n",
    "\n",
    "        logits = 1 - np.dot(constraints.T, lambdas)\n",
    "        shift = logits.max()\n",
    "        unnormalized_weights = np.exp(logits - shift)\n",
    "        norm = unnormalized_weights.sum()\n",
    "        weights = unnormalized_weights / norm\n",
    "\n",
    "        L = np.log(norm / self.n_samples) + shift - 1 + np.dot(lambdas, targets)\n",
    "        dL = targets - np.dot(constraints, weights)\n",
    "\n",
    "        if regularize:\n",
    "            L += 0.5 * np.sum(np.power(sigma_reg * lambdas, 2) + np.power(sigma_md * lambdas, 2))\n",
    "            dL += np.power(sigma_reg, 2) * lambdas + np.power(sigma_md, 2) * lambdas\n",
    "\n",
    "        return L, dL\n",
    "\n",
    "    def reweight(self,\n",
    "                 regularize: bool = False,\n",
    "                 sigma_reg: list = None,\n",
    "                 data_indices: list = None,\n",
    "                 store_result: bool = False\n",
    "                 ):\n",
    "\n",
    "        args = []\n",
    "\n",
    "        if data_indices is not None:\n",
    "            assert isinstance(data_indices, (np.ndarray, list)), \"data_indices must be type np.ndarray or list\"\n",
    "            data_indices = np.asarray(data_indices) if isinstance(data_indices, list) else data_indices\n",
    "            constraints, targets, lambdas0 = [getattr(self, i)[data_indices] for i in\n",
    "                                              [\"constraints\", \"targets\", \"lambdas0\"]]\n",
    "\n",
    "        else:\n",
    "            constraints, targets, lambdas0 = self.constraints, self.targets, self.lambdas0\n",
    "\n",
    "        args.extend([constraints, targets])\n",
    "\n",
    "        if regularize:\n",
    "            assert sigma_reg is not None or self.sigma_reg is not None, (\n",
    "                \"Must provide sigma_reg (regularization parameter)\"\n",
    "                \"as an argument or upon instantiation\")\n",
    "            args.extend([regularize,\n",
    "                         np.asarray(sigma_reg) if sigma_reg is not None else self.sigma_reg[data_indices].squeeze(),\n",
    "                         self.sigma_md[data_indices].squeeze()])\n",
    "\n",
    "        else:\n",
    "            args.extend([False, None, None])  # not necessary\n",
    "\n",
    "        result = minimize(\n",
    "            self.lagrangian,\n",
    "            lambdas0,\n",
    "            method='L-BFGS-B',\n",
    "            jac=True,\n",
    "            args=tuple(args)\n",
    "        )\n",
    "\n",
    "        weights = self.compute_weights(result.x, constraints)\n",
    "\n",
    "        if store_result:\n",
    "            if data_indices is not None:\n",
    "                warnings.warn(\"Storing parameters and weights from reweighting performed on a subset of the data.\")\n",
    "            self.lambdas = result.x\n",
    "            self.weights = weights\n",
    "            self.has_result = True\n",
    "\n",
    "        weighted_averages = constraints @ weights\n",
    "\n",
    "        return dict(lambdas=result.x,\n",
    "                    weights=weights,\n",
    "                    kish=self.compute_kish(weights),\n",
    "                    regularize=args[-2],\n",
    "                    sigma_reg=args[-1],\n",
    "                    data_indices=data_indices,\n",
    "                    weighted_averages=weighted_averages,\n",
    "                    targets=targets,\n",
    "                    rmse=rmse(weighted_averages, targets)\n",
    "                    )\n",
    "\n",
    "    def reset(self):\n",
    "        self.weights = None\n",
    "        self.lambdas = None\n",
    "        self.has_result = False\n",
    "        return\n",
    "\n",
    "    def compute_kish(self, weights: np.ndarray = None):\n",
    "        if weights is None:\n",
    "            assert self.weights is not None, \"Must provide weights if class attribute 'weights' is None\"\n",
    "            weights = self.weights\n",
    "        return 100 / (self.n_samples * np.power(weights, 2).sum())\n",
    "    \n",
    "    @conditional_ray(\"is_ray\")\n",
    "    def kish_scan_(self,\n",
    "                   data_indices: list = None,\n",
    "                   target_kish: float = None,\n",
    "                   sigma_reg_l: float = 0.001,\n",
    "                   sigma_reg_u: float = 20,\n",
    "                   steps: int = 200,\n",
    "                   scale: np.array = 1,\n",
    "                   store_sigma: bool = False):\n",
    "\n",
    "        if data_indices is not None:\n",
    "\n",
    "            assert isinstance(data_indices, (np.ndarray, list)), \"data_indices must be type np.ndarray or list\"\n",
    "            data_indices = np.asarray(data_indices) if isinstance(data_indices, list) else data_indices\n",
    "        else:\n",
    "            data_indices = np.arange(self.n_constraints)\n",
    "\n",
    "        if target_kish is not None:\n",
    "            self.target_kish = target_kish\n",
    "\n",
    "        kish = lambda sigma: self.reweight(regularize=True,\n",
    "                                           sigma_reg=sigma,\n",
    "                                           data_indices=data_indices,\n",
    "                                           store_result=False)[\"kish\"]\n",
    "        reached_target = False\n",
    "        sigma_optimal = sigma_reg_u * scale\n",
    "        for sigma in np.linspace(sigma_reg_l, sigma_reg_u, steps)[::-1]:\n",
    "\n",
    "            sigma = scale * sigma\n",
    "\n",
    "            if kish(sigma) < self.target_kish:\n",
    "                reached_target = True\n",
    "                break\n",
    "                \n",
    "            sigma_optimal = sigma\n",
    "\n",
    "        if not reached_target: print(\"Did not find optimal kish\")\n",
    "        if store_sigma: self.sigma_reg[data_indices] = sigma_optimal\n",
    "\n",
    "        return sigma_optimal\n",
    "    \n",
    "    def kish_scan(self,\n",
    "                  data_indices: list = None,\n",
    "                  target_kish: float = None,\n",
    "                  sigma_reg_l: float = 0.001,\n",
    "                  sigma_reg_u: float = 20,\n",
    "                  steps: int = 200,\n",
    "                  scale: np.array = 1,\n",
    "                  store_sigma: bool = False):\n",
    "        \n",
    "        \n",
    "        \n",
    "        args = dict(self=self,\n",
    "                    data_indices=data_indices,\n",
    "                    target_kish=target_kish,\n",
    "                    sigma_reg_l=sigma_reg_l,\n",
    "                    sigma_reg_u=sigma_reg_u,\n",
    "                    steps=steps,\n",
    "                    scale=scale,\n",
    "                    store_sigma=store_sigma\n",
    "                   )\n",
    "        \n",
    "        \n",
    "        if self.is_ray:\n",
    "            return self.kish_scan_().remote(**args)\n",
    "        else:\n",
    "            return self.kish_scan_()(**args)\n",
    "    \n",
    "\n",
    "    def optimize_sigma_reg(self,\n",
    "                           indices_list: list,\n",
    "                           single_sigma_reg_l: float = 0.001,\n",
    "                           single_sigma_reg_u: float = 20,\n",
    "                           single_steps: int = 200,\n",
    "                           global_sigma_reg_l: float = 0.01,\n",
    "                           global_sigma_reg_u: float = 20,\n",
    "                           global_steps: int = 60,\n",
    "                           multi_proc : bool = False\n",
    "                           ):\n",
    "\n",
    "        # regularization for each data type\n",
    "        if multi_proc:\n",
    "            \n",
    "            self.is_ray = True\n",
    "           \n",
    "            single_regs = np.asarray(ray.get([self.kish_scan(i,\n",
    "                                                  sigma_reg_l=single_sigma_reg_l,\n",
    "                                                  sigma_reg_u=single_sigma_reg_u,\n",
    "                                                  steps=single_steps)\n",
    "                                      for i in indices_list])).astype(float)\n",
    "            \n",
    "            single_regs = np.concatenate([np.ones(len(i)) * j for i, j in zip(indices_list, single_regs)])\n",
    "            \n",
    "            self.is_ray = False\n",
    "            \n",
    "                                \n",
    "        else:\n",
    "            single_regs = np.concatenate([self.kish_scan(i,\n",
    "                                                       sigma_reg_l=single_sigma_reg_l,\n",
    "                                                       sigma_reg_u=single_sigma_reg_u,\n",
    "                                                       steps=single_steps) * np.ones(len(i))\n",
    "                                      for i in indices_list])\n",
    "\n",
    "        # global regularization - find single scalar for regularization parameters of each data type\n",
    "        self.kish_scan(scale=single_regs,\n",
    "                       store_sigma=True,\n",
    "                       sigma_reg_l=global_sigma_reg_l,\n",
    "                       sigma_reg_u=global_sigma_reg_u,\n",
    "                       steps=global_steps,\n",
    "                       )\n",
    "        return\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fecd9b-bbf5-4f90-9b18-ea48488951c9",
   "metadata": {},
   "source": [
    " # provide the class with forward model data, corresponding experimental averages and target kish for regularization parameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67a1864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RMSE : 0.4068098357997516\n",
      " KISH : 13.056495810207366\n",
      " Time to find all regularization parameters and reweight 237 experimental constraints (macbook) : 26.48 seconds\n"
     ]
    }
   ],
   "source": [
    "reweight = MaxEntropyReweight(constraints,\n",
    "                              targets.flatten(),\n",
    "                              sigma_md=sigma_md,\n",
    "                              sigma_reg=None,#sigma_reg,\n",
    "                            target_kish = 10.0) #reweight class\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "reweight.optimize_sigma_reg(data_type_indices) # find regularization parameters\n",
    "\n",
    "result = reweight.reweight(regularize=True)\n",
    "\n",
    "print(f\" RMSE : {rmse(result['weighted_averages'], result['targets'])}\\n\",\n",
    "      f\"KISH : {result['kish']}\\n\",\n",
    "      f\"Time to find all regularization parameters and reweight {len(result[\"targets\"])} experimental constraints (macbook) : {round(time.time() - start, 2)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f0ddb-0cb3-4aed-8e7f-fc22e7ed271f",
   "metadata": {},
   "source": [
    "# The RMSE and KISH exactly match previous results !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
